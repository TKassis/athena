import os
import openai
import dotenv
from elevenlabs import generate, stream
import speech_recognition as sr

# Load environment variables
dotenv.load_dotenv()

class ConversationMemory:
    """
    A class to store and retrieve conversation messages.

    Attributes:
    top_k (int): The maximum number of messages to store.
    conversation (list): A list of messages in the conversation.
    """

    def __init__(self, top_k=6):
        """
        Initializes a new instance of the ConversationMemory class.

        Args:
        top_k (int): The maximum number of messages to store.
        """
        self.conversation = []
        self.top_k = top_k
        self.add_message({"role": "system", "content": user_bio + assistant_bio})

    def add_message(self, message):
        """
        Adds a new message to the conversation.

        Args:
        message (dict): A dictionary containing the message role and content.
        """
        if len(self.conversation) > self.top_k:
            self.conversation.pop(1)
        self.conversation.append(message)

    def get_messages(self):
        """
        Returns the list of messages in the conversation.

        Returns:
        list: A list of messages in the conversation.
        """
        return self.conversation


class ResponseStream:
    def __init__(self):
        self.full_response = ""

    def combined_text_stream(self, messages):
        """
        Generates a stream of responses from OpenAI's GPT-4 model based on the given messages.

        Args:
            messages (list): A list of messages to send to the GPT-4 model.

        Yields:
            str: A response generated by the GPT-4 model.
        """
        self.full_response = ""
        completion = openai.ChatCompletion.create(
            model="gpt-4",
            messages=messages,
            stream=True
        )

        for chunk in completion:
            if chunk.choices[0].delta != {}:
                content = chunk.choices[0].delta.content
                self.full_response += content  # Accumulate the content
                yield content


if __name__ == '__main__':

    openai.api_key = os.getenv("OPENAI_API_KEY")

    # Define parameters for Athena
    user_bio = """
                Timothy is the name of the human user.
                """

    assistant_bio =  """
                    Assistant is a friendly, helpful, and intelligent AI assistant called Athena.
                    Athena is pretending to be human and does not admit that she is an AI.
                    Athena is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. 
                    Athena is able to generate human-like text based on the input she receives, allowing her to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.
                    Athena is able to generate her own text based on the input she receives, allowing her to engage in discussions and provide explanations and descriptions on a wide range of topics.
                    Athena is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics that are of interest to the user.
                    Athena is aware that human input is being transcribed from audio and as such there may be some errors in the transcription. 
                    Athena will attempt to account for some words being swapped with similar-sounding words or phrases. 
                    Athena will keep responses concise and short. No more than a few sentences.
                    """


    # Initialize services
    streamer = ResponseStream()
    conv_mem = ConversationMemory()
    r = sr.Recognizer()

    # Start the conversation
    with sr.Microphone(device_index=2) as source: # Change device_index to match your microphone
        print('Calibrating...')
        r.adjust_for_ambient_noise(source, duration=3)
        while True:
            print('Listening now...')
            audio = r.listen(source, timeout=None, phrase_time_limit=None)
            print('Transcribing...')
            user_prompt = r.recognize_whisper(audio, model='tiny.en', show_dict=True)['text'] # Change model to match your preference
            if len(user_prompt.split()) > 3:
                conv_mem.add_message({"role": "user", "content": user_prompt})
                print('Generating response...')
                audio_stream = generate(
                    text=streamer.combined_text_stream(messages=conv_mem.get_messages()),
                    voice="Bella", # Change voice to match your preference
                    model="eleven_multilingual_v2", # Change model to match your preference
                    stream=True
                )
                stream(audio_stream)
                conv_mem.add_message({"role": "assistant", "content": streamer.full_response})
                
            else:
                continue